---
title: "EBA5002 Project"
author: "Team8 MyView"
date: "9/14/2020"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 3
---

## Introduction
Our project is focused on a home stay reservation company called Airbnb. Airbnb is a website for people to rent out their accommodation, offering a service of renting out houses or rooms on a short-term basis. The company has more than 3 million homes in 65,000 cities in 191 countries. Our project objective is to use the historic booking data and details about homes and customer behaviours to find out the problems that the company faced, so as to use analytics skills and predictive to tackle the problem and help Airbnb Beijing expand their market.The potential approaches we consider include random forest and neural network.

## Contributions
Xu Haotian:
Li Keqi:
Huang Jianhao: 
Wen Jingtian: 
Ye Ruchen:
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# --> Modify to your working directory before run <--
knitr::opts_knit$set(base.dir = '~/CloudFiles/EBAC_Files/Group_Projects/EBA5002/')

library(tidyverse)
library(mice)
library(VIM)
library(leaflet)
library(ggplot2)
library(mapdeck)
library(dplyr)
library(caTools)
library(lmtest)
library(rpart)
library(keras)
library(leafletCN)
library(leaflet.providers)
library(jsonify)
library(colourvalues)
library(lubridate)
library(scales)
library(shiny)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
library(tmap)

```

## Description of Data

The data is sourced from the **Kaggle** website ` https://www.kaggle.com/merryyundi/airbnb-beijing-20190211` which hosts publicly available data.

## The dataset comprises of three main tables:

* `listings` - Detailed listings data showing 106 atttributes for each of the listings. Some of the attributes used in the analysis are `price` (continuous), `longitude` (continuous), `latitude` (continuous), `room_type` (categorical), `is_superhost` (categorical), `neighbourhood_cleanse` (categorical), `bedrooms` (categorical) among others.

* `reviews` - Detailed reviews given by the guests with 6 attributes. Key attributes include `date` (datetime), `listing_id` (discrete), `reviewer_id` (discrete) and `comment` (textual).

* `calendar` - Provides details about booking for the next year by listing. Seven attributes in total including `listing_id` (discrete), `date` (datetime), `available` (categorical) and `price` (continuous).

## Analysis of data quality

The data quality of the data is not perfect. There are more than half missing data in columns'square_feet','weekly_price',  'monthly_price', 'security_deposit','cleaning_fee' and about 40% of the data in the feature about reviews. We had to drop this column. For other columns that are relatively complete, we need to perform a few imputations and transformations on our dataset for us to create the desired visualizations. Some of the column in file 'Listings' miss a lot of data, some of them contain outliers, and most of the columns/features we were interested in did not contain data in the required format and hence were manipulated in a way that their meanings are retained. 



#### Key Feature Engineering

1. ```comment (reviews)```: We extensively used this feature in our analysis. The dataset contained reviews in multiple languages such as Chinese, Spanish, and English which made it difficult for it to be analyzed. We subsetted the data to include only the reviews that were in English and Chinese, and performed text filtering to remove common stop words and phrases that do not significantly contribute to the meaning of the review.And then we conduct wordcloud to discover what kinds of features or services about the property matter to customers.

2.``` price,extra_people(listings, calendar)```: The price column contained data in string format with the currency symbol ‘$’ and comma separator ‘,’ attached to it. This column was manipulated to contain integer values for analysis. And the feature extra_people need to be transformed in the same way. And this is the targeted variables of the predictive model.

3. ```date (calendar, listings, reviews)```: The date was contained in mm-dd-yyyy format. They are displayed as character so they need to be transfered to date format. Furthermore, it was transformed multiple times during the analysis to obtain weekly, monthly or yearly insights.

## Read data and quick look
```{r}
listings <- read.csv('listings.csv', stringsAsFactors = F, encoding = 'UTF-8', na.strings = c('N/A'))

names(listings)
str(listings)
# summary(listings)
```

## Data cleaning
### Select useful columns
Data in the listing files is the main data we use to build predictive model. There are 106 columns in this dataset and of course there is no way that we can put them all in the model because of the existence of some meaningless feature. So after the preview, we select 45 columns that we think might be useful to do the data cleaning and transformation.
```{r}
useful_columns <- c('id','host_id','host_since','host_acceptance_rate',"host_is_superhost", 'host_listings_count','host_verifications','host_has_profile_pic','host_identity_verified', 'street','neighbourhood','neighbourhood_cleansed','zipcode','market','latitude','longitude','is_location_exact','property_type', 'room_type','accommodates','bathrooms','bedrooms','beds','bed_type','square_feet','price','weekly_price','monthly_price', 'security_deposit','cleaning_fee','guests_included','extra_people','minimum_nights','maximum_nights','availability_365', 'number_of_reviews','review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin', 'review_scores_communication','review_scores_location','review_scores_value')
length(useful_columns)
listings <- listings[, useful_columns]
```

### Dealing with Missing Values
The selected data also had null values. To preserve all the information, we imputed or dropped the rows and columns containing null values while conducting exploratory analysis that made use of these features.

We construct a aggr graph(we are sorry that the graph dosen't display all the column names because their length of name are too long) to analyze the missing values for the variables that we would be using in our exploratory analysis.

Several observation from the graph:

* Most of the rows have no missing values. 
* Columns 'square_feet','weekly_price','monthly_price','security_deposit','cleaning_fee' are missing more than half of the data, and columns 'review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value' are missing 40% of the data.

We can conclude from the above graph that the Airbnb dataset contains several missing value which will affect our analysis if not handle well .For datasets that containing more than 40% missing value, we choose to drop them.And for the rest columns, we use function "complete.case" to drop the row because their volume are small and will not affect the predicting accauracy very much.

Check the missing values:
```{r}
aggr(listings)
md.pattern(listings)

#Drop the columns that contain too much missing data
dropcol <- c('neighbourhood','zipcode','market','square_feet','weekly_price','monthly_price','security_deposit','cleaning_fee','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value','host_acceptance_rate','host_response_rate')
listings[, dropcol] = NULL

aggr(listings)

listings = listings[complete.cases(listings), ]
str(listings)

aggr(listings)


```

### Deal with single variables
Some columns need to be deal with individually. The "date" and "host_since" column need to transformed into date format so that we can use it as time feature. The columns "price" and "extra_people" are in character format containing "$" and ",", so we drop these special symbols and transform them into numeric data. Column "host_listings_time" contains 15 zero values, which is not in accord with reality, so these row need to be drop.

```{r}
# Date
listings$host_since <- as.Date(listings$host_since)

# Host_since
listings$host_since <- as.Date(listings$host_since)

# Price 
listings$price = gsub(',', '', listings$price)
listings$price = substring(listings$price,2,nchar(listings$price))
listings$price = as.numeric(listings$price)
str(listings$price)

# Extra people 
listings$extra_people = gsub(',', '', listings$extra_people)
listings$extra_people = substring(listings$extra_people, 2, nchar(listings$extra_people))
listings$extra_people = as.numeric(listings$extra_people)
str(listings$extra_people)


#Host_listing_time
dim(filter(listings, listings$host_listings_count == 0))
listings = filter(listings, listings$host_listings_count != 0)

```

### Deal with outliers
We draw a histgram and boxplot for the cleaned numeric data to view their distribution and outliers. Through these graph, we found that the columns 'host_listings_count','accommodates','bathrooms','bedrooms','beds' and 'price' are obviously positive skewed and containing many outliers. So we decided that the data that are outside the 1.5 IQR plus 75 percentile are outliers and use the 95th percentile to replace them. Other columns need to be deal with individually to simulate real situation better. For columns "extra_people", we thought that the 95th percentile is too small and cannot reflect all the situation because there are lots of room charging high cost for extra people. So we set a top price $500 and replaced the data biggest than this price. For columns "minimun_night" and "maximun_night", we thought that minimnn night will not larger than 365 and maximun will not larger than 1125, so we set them as the cap.
```{r fig.height=3.5, fig.width=3.5}
num_variable = c('host_listings_count','latitude','longitude','accommodates','bathrooms','bedrooms','beds','price','guests_included', 'extra_people','minimum_nights','maximum_nights','availability_365','number_of_reviews')
par(mfrow = c(4, 4))
for (i in num_variable){
  x = listings[,i]
  print(ggplot(listings, aes(x = x)) + geom_histogram(stat = 'count') + ggtitle(label = i))
}

for (i in num_variable){
  x = listings[, i]
  print(ggplot(listings, aes(x = x)) + geom_boxplot() + ggtitle(label = i))
}

remove_outliers = c('host_listings_count','accommodates','bathrooms','bedrooms','beds','price')

# Replace values
replace_outliers = function(data) {
  qnt = quantile(data, probs = c(.25,.75), na.rm = T)
  cap = quantile(data, probs = c(.05,.95), na.rm = T)
  Th = 1.5 * IQR(data, na.rm = T)
  cleandata = ifelse((data < (qnt[1] - Th)), cap[1], data)
  cleandata = ifelse((data > (qnt[2] + Th)), cap[2], data)
  return(cleandata)
}
for (i in remove_outliers){
  listings[,i] <- replace_outliers(listings[, i])
}
#replace column extra_people individually 
listings$extra_people = ifelse(listings$extra_people >500, 500, listings$extra_people)


#replace column minimun night, maximum night individually 
listings$minimum_nights = ifelse(listings$minimum_nights > 365, 365, listings$minimum_nights)
listings$maximum_nights = ifelse(listings$maximum_nights > 1125, 1125, listings$maximum_nights)

#replace column bathrooms,bedrooms,beds individually
a = c('bathrooms','bedrooms','beds')
for (i in a){
  x = listings[,i]
  listings[,i] = ifelse(listings[,i] < 1, 1, listings[,i])
}

#replot the data to check 
for (i in num_variable){
  x = listings[, i]
  print(ggplot(listings, aes(x = x)) + geom_boxplot() + ggtitle(label = i))
}

#Combine some small district 
listings %>% group_by(neighbourhood_cleansed) %>% summarise(Freq = n()) %>% arrange(desc(Freq))

listings$neighbourhood_cleansed = ifelse(listings$neighbourhood_cleansed == "石景山区 / Shijingshan", "房山区 / Fangshan", listings$neighbourhood_cleansed)
listings$neighbourhood_cleansed = ifelse(listings$neighbourhood_cleansed == "门头沟区 / Mentougou", "房山区 / Fangshan", listings$neighbourhood_cleansed)
listings$neighbourhood_cleansed = ifelse(listings$neighbourhood_cleansed == "平谷区 / Pinggu", "密云县 / Miyun", listings$neighbourhood_cleansed)

#Combine property type 
listings %>% group_by(property_type) %>% summarise(Freq = n()) %>% arrange(desc(Freq))
```

### Discover relation between variables
We draw histograms and boxplots for every numerical variables. We found that the distributions of several variables, including "host_listings_count"and "price", are positive skewed, which is consistent with our common sense because the majority of the rental housing are cost-effective, while some luxury houses that are well above the average market price also exist. We tried to logarithm these features, while the result of the linear regression model is even worse, so we just keep them in original format.Some special variables need to be deal with individually.
```{r}
#host since 
listings$host_since_year = year(listings$host_since)
str(listings$host_since_year)
listings$datecut = cut(listings$host_since_year,breaks = c(2009, 2012, 2014, 2016, 2018, 2020),
                             labels = c('2010-2012','2012-2014','2014-2016','2016-2018','After2018'))
ggplot(listings,aes(x=datecut,y = price,fill = datecut)) + geom_boxplot() + scale_y_continuous(limits = c(0, 8000))+ggtitle(label = 'Price at different hosting time')


#host_listing_count
ggplot(listings, aes(x = host_listings_count)) + geom_histogram() 
listings$host_listings_cut = cut(listings$host_listings_count,breaks = c(0, quantile(listings$host_listings_count,0.2), 
                                                                         quantile(listings$host_listings_count,0.4), 
                                                                         quantile(listings$host_listings_count,0.6),
                                                                         quantile(listings$host_listings_count,0.8),
                                                                        quantile(listings$host_listings_count,1)))
ggplot(listings,aes(x=host_listings_cut,y = price,fill = host_listings_cut)) + geom_boxplot()+ggtitle(label = 'Price at different hosting listing count')


#Location 
listings$latitude_cut = cut(listings$latitude,breaks = c(0, quantile(listings$latitude,0.2), 
                                                                         quantile(listings$latitude,0.4), 
                                                                         quantile(listings$latitude,0.6),
                                                                         quantile(listings$latitude,0.8),
                                                                        quantile(listings$latitude,1)))
listings$longitude_cut = cut(listings$longitude,breaks = c(0, quantile(listings$longitude,0.2), 
                                                                         quantile(listings$longitude,0.4), 
                                                                         quantile(listings$longitude,0.6),
                                                                         quantile(listings$longitude,0.8),
                                                                        quantile(listings$longitude,1)))

ggplot(listings,aes(x = latitude_cut, y = longitude_cut,fill=price))+geom_tile()+ggtitle(label = "Location")+scale_fill_gradient2(low ="yellow", high ="red", mid ="orange")

#Guest included
listings$guests_included_cut = cut(listings$guests_included,breaks = c(0,3,6,9,12,16),labels = c('0-3','3-6','6-9','9-12','12-16'))
ggplot(listings,aes(x=guests_included_cut,y = price,fill = guests_included_cut)) + geom_boxplot()+ggtitle(label = 'Guest Included')

#Extra people
ggplot(listings, aes(x = extra_people)) + geom_histogram() 
listings$extra_people_cut = cut(listings$extra_people,breaks = c(-1, quantile(listings$extra_people,0.72), 
                                                                         quantile(listings$extra_people,0.8),
                                                                         quantile(listings$extra_people,0.9),
                                                                        quantile(listings$extra_people,1)))

ggplot(listings,aes(x=extra_people_cut,y = price,fill = extra_people_cut)) + geom_boxplot()+ggtitle(label = 'Extra_people')

#Number of reviews
ggplot(listings, aes(x = number_of_reviews)) + geom_histogram() 
listings$number_of_reviews_cut = cut(listings$number_of_reviews,breaks = c(-1, quantile(listings$number_of_reviews,0.5), 
                                                                         quantile(listings$number_of_reviews,0.7),
                                                                         quantile(listings$number_of_reviews,0.8),
                                                                        quantile(listings$number_of_reviews,1)))

ggplot(listings,aes(x=number_of_reviews_cut,y = price,fill = number_of_reviews_cut)) + geom_boxplot()+ggtitle(label = 'Number of reviews')

#Other feature
plot_column = c("host_is_superhost","host_identity_verified","neighbourhood_cleansed",
                "property_type","room_type","accommodates","bathrooms","bedrooms","beds","bed_type")

for (i in plot_column){
  x = as.character(listings[, i])
  print(ggplot(listings, aes(x = x,y = price, fill = x)) + geom_boxplot() + ggtitle(label = i))
}

```

## Visualize
In this section, we draw some plots to explore the data set from different aspects.

### Demand in different distircts and property types
Firstly, we should deal with the neighbourhood column due to type of it is not the same. We extract the chinese names from this column and create a new column called district, to satisfy the requirement of geographic visualization. Then we modify the column itself to ensure the format of it is the same with "chinese name / english name" of the districts.
```{r}
library(stringr)
library(leafletCN)
cleanDistrict <- function(s) {
  return(str_extract(s, "[\\p{Han}]+"))
}
listings$district <- sapply(listings$neighbourhood_cleansed, cleanDistrict)
unique(listings$neighbourhood_cleansed)
fixErrorInNeibourhood <- function(s) {
  if (s == '东城区') return('东城区 / Dongcheng')
  if (s == '西城区') return('西城区 / Xicheng')
  if (s == '海淀区') return('海淀区 / Haidian')
  if (s == '房山区') return('房山区 / Fangshan')
  if (s == '石景山区') return('石景山区 / Shijingshan')
  if (s == '昌平区') return('昌平区 / Changping')
  return(s)
}
listings$neighbourhood_cleansed <- sapply(listings$neighbourhood_cleansed, fixErrorInNeibourhood)
unique(listings$neighbourhood_cleansed)
```

Then we draw a interactive map showing all the houses in the listing. We can use this map to check the exact location of each house, and its host ID, rating and price etc.
```{r}
leaflet(listings) %>%
  addTiles() %>%
  addMarkers(~longitude, ~latitude, labelOptions = labelOptions(noHide = F), clusterOptions = markerClusterOptions(), popup = paste0("<b> Host ID: </b>", listings$host_id , "<br/><b> Rating: </b>", listings$review_scores_rating, "<br> <b> Price: </b>", listings$price, "<br/><b> Room Type: </b>", listings$room_type, "<br/><b> Property Type: </b>", listings$property_type)) %>% 
  setView(lat = 39.9, lng = 116.38, zoom = 10) %>%
  addProviderTiles("CartoDB.Positron")
```

Then we try to get ideas on the summarized data grouped by each district on the map. We draw the map of average price, average rating and house counts in each district, and get the figure as following:
```{r}
mean_price <- listings %>% group_by(district) %>% summarise(mean_price = mean(price)) %>% arrange(desc(mean_price))
mean_price$district <- as.character(mean_price$district)

print(mean_price)

beijing_districs <- data.frame(regionNames("北京"))
colnames(beijing_districs) <- "district"
geo_data <- full_join(beijing_districs, mean_price)
map <- leafletGeo("北京", geo_data)
pal <- colorNumeric(palette = "Blues", domain = map$value)

leaflet(map) %>% amap() %>%
    addPolygons(stroke = TRUE,
                smoothFactor = 1,
                fillOpacity = 0.7,
                weight = 1,
                color = ~pal(value),
                popup = ~htmltools::htmlEscape(popup)
    ) %>%
    addLegend("bottomright", pal = pal, values = ~value,
              title = "Average Price by Districts",
              labFormat = leaflet::labelFormat(prefix = ""),
              opacity = 1)
```

```{r}
mean_rating <- listings %>% group_by(district) %>% summarise(mean_rating = mean(review_scores_rating, na.rm = T)) %>% arrange(desc(mean_rating))
mean_rating$district <- as.character(mean_rating$district)

print(mean_rating)

geo_data <- full_join(beijing_districs, mean_rating)
map <- leafletGeo("北京", geo_data)
pal <- colorNumeric(palette = "Blues", domain = map$value)

leaflet(map) %>% amap() %>%
    addPolygons(stroke = TRUE,
                smoothFactor = 1,
                fillOpacity = 0.7,
                weight = 1,
                color = ~pal(value),
                popup = ~htmltools::htmlEscape(popup)
    ) %>%
    addLegend("bottomright", pal = pal, values = ~value,
              title = "Average Rating by Districts",
              labFormat = leaflet::labelFormat(prefix = ""),
              opacity = 1)

```

```{r}
count_houses <- listings %>% group_by(district) %>% summarise(count = n()) %>% arrange(desc(count))
count_houses$district <- as.character(count_houses$district)

print(count_houses)

geo_data <- full_join(beijing_districs, count_houses)
map <- leafletGeo("北京", geo_data)
pal <- colorNumeric(palette = "Blues", domain = map$value)

leaflet(map) %>% amap() %>%
    addPolygons(stroke = TRUE,
                smoothFactor = 1,
                fillOpacity = 0.7,
                weight = 1,
                color = ~pal(value),
                popup = ~htmltools::htmlEscape(popup)
    ) %>%
    addLegend("bottomright", pal = pal, values = ~value,
              title = "Houses Counts by Districts",
              labFormat = leaflet::labelFormat(prefix = ""),
              opacity = 1)

```
From the three figures above, we find that the average price is lower in the center of the city, and higher in the suburb, which is just the opposite to our assumption at first. We can also get that the average rating is approximately correlated with the average price of the house rented. We see higher average prices in the suburbs of Beijing, which are located in the east and north Beijing, and obvious higher rating among them compared to the center of the city. In the last figure above, we can find a significant difference in the number of houses listed on Airbnb between the center of city and suburbs. The number of houses listed in the city center are many times of that in other districts.

After the exploration on the summarized data on each district, we are interested in how the data distributed with more precised locations on the map, so we draw the following two maps, which shows more precised location of the data.
```{r}
set_token("pk.eyJ1IjoidG9tbXl4dTk3IiwiYSI6ImNrZjUzMnJjczAxMGYycW96ZGhhN3kybzgifQ.eZ1HnCaedPi6Yj-n-I3H7w")
mapdeck(style = mapdeck_style('light'), pitch = 30, zoom = 10) %>%
  add_heatmap(
    data = listings
    , lat = "latitude"
    , lon = "longitude"
    , weight = "price"
    , layer_id = "heatmap_layer"
    , colour_range = colourvalues::colour_values(1:6, palette = colourvalues::get_palette("viridis")[70:256,])
  )
```

```{r}
mapdeck(style = mapdeck_style('light'), pitch = 45, zoom = 10) %>%
  add_hexagon(
    data = listings
    , lat = "latitude"
    , lon = "longitude"
    , layer_id = "hex_layer"
    , elevation_scale = 100
    , legend = TRUE
    , colour_range = colourvalues::colour_values(1:6, palette = colourvalues::get_palette("viridis")[70:256,])
  )
```
Among these two figures, the first one is a heat map which shows the price of the houses, and the second one is a hexagon map which shows the number of houses in a small hexagon area. From the map we can nearly get the same conclusion as the previous part, which generally states lower price and more houses in the city center, and higher price and fewer houses in the suburbs. To find out the potential reason behind what the data represent to us by the map, we decide to explore some more with the statistical graphs.

We are going to explore the difference ratios of property types, we select three districts in the city center and three in the suburb. For the types of the houses, we group by property_type and summarize it, then select the top 5 of them.
```{r}
listings %>% group_by(property_type) %>% summarise(counts = n()) %>% arrange(desc(counts))
```

```{r}
selected_districts <- c("朝阳区 / Chaoyang","东城区 / Dongcheng","海淀区 / Haidian", "怀柔区 / Huairou", "延庆县 / Yanqing", "门头沟区 / Mentougou")
center_city <- c("朝阳区 / Chaoyang","东城区 / Dongcheng","海淀区 / Haidian")
selected_property_type <- c("Apartment", "House", "Condominium", "Serviced apartment", "Loft")

head(listings)
a =listings %>% group_by(host_is_superhost)%>% summarise(mean = mean(price))

sum = summarise(a,sum(Freq))
a = a %>% mutate(ratio = Freq/as.numeric(sum))
a
sum = summarise(a,sum(Freq))
as.numeric(sum)
a1 = a[1:6,]
a1
```






```{r}
selected_districts2 <- c("朝阳区 / Chaoyang","东城区 / Dongcheng","海淀区 / Haidian", "西城区 / Xicheng", "丰台区 / Fengtai", "通州区 / Tongzhou")

selected_districts3 <- c("怀柔区 / Huairou","房山区 / Fangshan","延庆县 / Yanqing", "石景山区 / Shijingshan", "门头沟区 / Mentougou", "平谷区 / Pinggu")

property <- listings %>% group_by(neighbourhood_cleansed, property_type) %>% summarise(Freq = n())
property <- property %>% filter(property_type %in% selected_property_type) %>% filter(neighbourhood_cleansed %in% selected_districts2) %>% mutate(city_center_or_subrub = ifelse(neighbourhood_cleansed %in% center_city, 1, 0))

total_property <- listings %>% filter(property_type %in% selected_property_type) %>% filter(neighbourhood_cleansed %in% selected_districts2) %>% group_by(neighbourhood_cleansed) %>% summarise(sum = n()) %>% mutate(city_center_or_subrub = ifelse(neighbourhood_cleansed %in% center_city, 1, 0))

property_ratio <- merge(property, total_property, by="neighbourhood_cleansed")
property_ratio <- property_ratio %>% mutate(ratio = Freq/sum) 

ggplot(a, aes(x = host_is_superhost, y = mean, fill = host_is_superhost)) +
  geom_bar(stat="identity")  +
  ggtitle("Score of reviews of different hosts") +
  xlab("neighbourhood_cleansed") + ylab("Price")

```



```{r}
property <- listings %>% group_by(neighbourhood_cleansed, property_type) %>% summarise(Freq = n())
property <- property %>% filter(property_type %in% selected_property_type) %>% filter(neighbourhood_cleansed %in% selected_districts) %>% mutate(city_center_or_subrub = ifelse(neighbourhood_cleansed %in% center_city, 1, 0))

total_property <- listings %>% filter(property_type %in% selected_property_type) %>% filter(neighbourhood_cleansed %in% selected_districts) %>% group_by(neighbourhood_cleansed) %>% summarise(sum = n()) %>% mutate(city_center_or_subrub = ifelse(neighbourhood_cleansed %in% center_city, 1, 0))

property_ratio <- merge(property, total_property, by="neighbourhood_cleansed")
property_ratio <- property_ratio %>% mutate(ratio = Freq/sum)

ggplot(property_ratio, aes(x = neighbourhood_cleansed, y = ratio, fill = property_type)) +
  geom_bar(position = "dodge", stat = "identity") + xlab("District") + ylab("Ratio") +
  scale_fill_discrete(name = "Property Type") + 
  scale_y_continuous(labels = scales::percent) +
  ggtitle("Different Property Ratio in City Center and Suburb") +
  theme(text = element_text(family = "AdobeSongStd-Light")) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6)) +
  theme(plot.caption = element_text(color = "grey68")) + scale_color_gradient(low = "#d3cbcb", high = "#852eaa") +
  scale_fill_manual("Property Type", values = c("#e06f69","#357b8a", "#7db5b8", "#59c6f3", "#f6c458")) +
  xlab("District") + ylab("Percentage")

ggplot(property_ratio, aes(x = neighbourhood_cleansed, y = sum, fill = neighbourhood_cleansed)) +
  geom_bar(position = "dodge", stat="identity") + 
  scale_fill_discrete(name = "Districts") +
  ggtitle("House Counts in Six Different Districts") +
  theme(text = element_text(family = "AdobeSongStd-Light")) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6)) +
  xlab("District") + ylab("Count")
  
```

```{r fig.width=5}
average_price_by_type <- listings %>% group_by(property_type, neighbourhood_cleansed) %>% summarise(avg_price = mean(price))
average_price_by_type <- average_price_by_type %>% filter(property_type %in% selected_property_type) %>% filter(neighbourhood_cleansed %in% selected_districts)

ggplot(average_price_by_type, aes(x = property_type, y = avg_price, fill = property_type)) +
  geom_bar(position = "dodge", stat="identity") +
  facet_grid(~ neighbourhood_cleansed) +
  ggtitle("Average Price by Property Type in Different Districts") +
  theme(text = element_text(family = "AdobeSongStd-Light")) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6)) +
  xlab("District") + ylab("Price")
```
From the three plots above, we come with some conclusions:

- Chaoyang district contain the largest part of the property, much larger than other districts. The maximum apartment style listings are located in Chaoyang district, constituting more than 50% of all properties in that neighborhood. Haitian district is in the second place. As we know, these are economically developed regions.

- There is a significant difference in composing of properties between the two kinds of area. In the center city, we can find obviously that apartments are much more than other types of properties, in the same time, a whole set of house and loft house are provided with more ratio in the suburb areas.

- The average prices of all kinds of property are general higher in suburb, bu we find houses and loft properties are more higher than other types.

So for this part, we can infer that:

- There is a positive correlation between ratings and prices, higher the price of the property, higher rating it may receive in the customers' reviews. And this makes sense in our common brief.

- The reason why the counts is high and average price is lower in the center of the city is that, there are more needs in there so the amount of property is much higher, and the property comes with more common service and facilities, therefore they can provide a more affordable price for most people, and this is just the reason why rating can be lower. But in the suburb, there are less needs, and people are more likely to rent a house for there vacation, so they will provide more convenient services, in the same time higher price and higher rating come due to that.

### Demand in different times
In this section, we conduct analysis on the demand in different times. We will look at demand over the years since the inception of Airbnb in 2010 and across months of the year to understand the development of Airbnb and the the seasonality of the demand with customers.

We decide to read the reviews data because we don't have the exact transaction log of the houses. According to our experience of using Airbnb, there are approximately more than half of the customers will leave a review when they check out, so we think that the number of reviews is a perfect representative variable of the actual transaction amounts. We read the reviews data and do the necessary cleaning firstly, and then conduct to the analysis.
```{r}
reviews = read.csv("reviews.csv", stringsAsFactors = F,encoding = 'UTF-8', na.strings = c('N/A'))
reviews$date = as.Date(reviews$date)
```

```{r}
reviews_num <- reviews %>% group_by(date = date) %>% summarise(number = n())

ggplot(reviews_num, aes(date, number)) +
  geom_point(na.rm=TRUE, color = "#007A87", alpha=0.5) + 
  geom_smooth(color = 'red') +
  ggtitle("The demonds of Airbnb among several years") +
  labs(x = "Year", y = "Number of reviews") 
```
We find that the number of reviews increased rapidly over the recent years, which indicates the same scope of increasement in the demands. Also we can find obvious seasonality in the demands.

We choose the data in 2017 and 2018 seperately, and to find more about the seasonality.
```{r}
review_num_2017 <- reviews_num %>% filter(year(date) == 2017)
review_num_2018 <- reviews_num %>% filter(year(date) == 2018)

ggplot(review_num_2017, aes(date, number)) +
  geom_point(na.rm=TRUE, color = "#007A87", alpha=0.5) + 
  geom_smooth(color = 'red') +
  ggtitle("The demonds of Airbnb in 2017") +
  labs(x = "Year", y = "Number of reviews") 

ggplot(review_num_2018, aes(date, number)) +
  geom_point(na.rm=TRUE, color = "#007A87", alpha=0.5) + 
  geom_smooth(color = 'red') +
  ggtitle("The demonds of Airbnb in 2018") +
  labs(x = "Year", y = "Number of reviews") 
```
From the plots above, we get the conclusion that, there are less demands for Airbnb houses in the first half of the year, and around July to October the demands reach the peek in a year, probably due to the summer vacation of the school and the flourish of tourism at that time.

## Apply models and analyse
Setting x, y and data preparing.
```{r}
# Specify outcome variable and independent variables and the using data here
# -----------------------------------------------
outcome <- "price"
variables <- c("host_listings_count","guests_included","extra_people","neighbourhood_cleansed","property_type","room_type","accommodates","bathrooms","bedrooms","beds","bed_type")
data <- listings
# -----------------------------------------------

f <- as.formula(paste(outcome, paste(variables, collapse = " + "), sep = " ~ "))

# Strength of correlation
cor(listings[c(outcome, variables)] %>% select_if(is.numeric))
# Scatter plot matrix
pairs(as.formula(paste("", paste(c(num_variable, outcome), collapse = " + "), sep = "~")), 
      data = data, main = "Scatter Plot Matrix")

split_data <- sample.split(data[, outcome], SplitRatio = 0.9)
train_set <- data[split_data,]
test_set <- data[!split_data,]
```

### Linear Regression
```{r}
# Fit model
start <- lm(f, data = train_set)
smallest <- formula(lm(as.formula(paste(outcome, "1", sep = " ~ ")), data = train_set))
lr_model <- step(start, direction = "backward", trace = FALSE, scope = smallest)
summary(lr_model)

# Residuals check
plot(lr_model)

# Auto-correlation test ~ Durbin Watson test
dwtest(lr_model)

# Model accuracy comparison using test/ train data
res_test <- predict(lr_model, test_set) - test_set[outcome]
plot(res_test$price)
hist(res_test$price)

sqrt(mean(res_test$price^2))
```

### Decision tree
```{r}
tree_model <- rpart(f, data = data, method = "anova")

pr <- (predict(tree_model, test_set) - test_set[outcome]) / test_set[outcome]
sqrt(mean(pr$price^2))
```

### Nerual Network
```{r}
# Check if Keras is available
is_keras_available()

# Split X and Y of data
x_train <- train_set[variables]
y_train <- train_set[outcome]
x_test <- test_set[variables]
y_test <- test_set[outcome]

# Creating the sequential model
nn_model <- keras_model_sequential() %>%   
  layer_dense(units = 6, activation = "relu", input_shape = ncol(x_train)) %>%
  layer_dense(units = 4, activation = "relu") %>%
  layer_dense(units = ncol(y_train))
summary(nn_model)

# Train model
compile(nn_model, loss = "mse", optimizer = optimizer_sgd(), metrics = "mae")
history <- fit(nn_model, data.matrix(x_train), data.matrix(y_train), epochs = 20, batch_size = 32, verbose = 1)
plot(history)

# Perform prediction
y_pred <- predict(nn_model, data.matrix(x_test))

result_mae <- mean(abs(y_pred - data.matrix(y_test)))
result_mae
```

```{r}
#wordcloud

sampledreviews <- read.csv('english_comments.csv')
splitsampledreviewscoloumn <- unlist(strsplit(as.character(sampledreviews$comments), split=" "))
reviewsWordDF <- data.frame("word" = splitsampledreviewscoloumn)
wordDF <- reviewsWordDF %>% count(word,sort = TRUE) %>% ungroup()

docs <- Corpus(VectorSource(splitsampledreviewscoloumn))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs = tm_map(docs,removePunctuation)
docs <- tm_map(docs, removeWords, c(' ',"we","it", "overall", "this", "airbnb", "thanks", "also","is", "the","of","well","beijing","great","one","really",'and','to','nice'))
newcorpusdf <- data.frame(text=sapply(docs, identity),stringsAsFactors=F)
newcorpusdffiltered <- newcorpusdf %>% filter(text != "")
wordDF <- newcorpusdf %>% count(text, sort = TRUE) %>% ungroup()

set.seed(789)
wordcloud(words = wordDF$text,
          freq = wordDF$n,
          min.freq = 1000,
          max.words=500, colors = c("#e06f69","#357b8a", "#7db5b8", "#59c6f3"))

r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
library('text2vec')
tokens <- space_tokenizer(as.character(sampledreviews$comments))
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vectorizer <- vocab_vectorizer(vocab)
# use window of 5 for context words
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)

glove = GlobalVectors$new(rank = 50, x_max = 20,learning_rate = 0.15,alpha = 0.75,
                          lambda = 0.0, shuffle = FALSE)

# `glove` object will be modified by `fit()` call !
word_vectors = glove$fit_transform(tcm, n_iter = 20)
word_vectors1 <- glove$components
```


## Using model for making decisions
```{r}
c("host_listings_count","guests_included","extra_people","neighbourhood_cleansed","property_type","room_type","accommodates","bathrooms","bedrooms","beds","bed_type")
neighbourhood_cleansed = dput(unique(listings$neighbourhood_cleansed))
property_type = dput(unique(listings$property_type))
room_type = dput(unique(listings$room_type))
bed_type = dput(unique(listings$bed_type))
ui <- fluidPage(
  titlePanel("Host Assistance System"),
  sidebarLayout(
    sidebarPanel(
       selectInput(inputId = "neighbourhood_cleansed",
                  label = "Select the district of your house:",
                  choices = neighbourhood_cleansed),
       selectInput(inputId = "property_type",
                   label = "Select the property type",
                   choices = property_type),
       selectInput(inputId = "room_type",
                   label = "Select the room type",
                   choices = room_type),
       numericInput(inputId = "host_listings_count",
                    label = "How many house do you list?",
                    value = 0),
       numericInput(inputId = "guests_included",
                    label = "How many guests do you restrict for living?",
                    value = 0),
       numericInput(inputId = "extra_people",
                    label = "How muck do you charge for extra people?",
                    value = 0),
       numericInput(inputId = "accommodates",
                    label = "How many people can you house accommodates?",
                    value = 0),
       sliderInput(inputId = "bathrooms",
                   label = "How many bathrooms are there in your house?",
                   min = 0,
                   max = 5,
                   value = 0),
        sliderInput(inputId = "bedrooms",
                   label = "How many bedrooms are there in your house?",
                   min = 0,
                   max = 5,
                   value = 0),
        sliderInput(inputId = "beds",
                   label = "How many beds are there in your house?",
                   min = 0,
                   max = 8,
                   value = 0),
        selectInput(inputId = "bed_type",
                   label = "Select the bed type",
                   choices = bed_type)
    ),
    mainPanel(
      tabsetPanel(type = "tabs",
                  tabPanel("Price Prediction",textOutput("text1"),
      verbatimTextOutput("low_result"),
      textOutput("text3"),
      tableOutput("count"),
      textOutput("text4"),
      tableOutput("type")),
      tabPanel("Property Detail Map",leafletOutput("map")),
      tabPanel("Price Heatmap",mapdeckOutput("heat"))
      )
    )
  )
)

server <- function(input,output)({
    output$text1 = renderText({
      "Recommended Price:"
    })
    output$low_result <- renderText({
       predict(lr_model,data.frame(neighbourhood_cleansed = input$neighbourhood_cleansed,property_type = input$property_type,
           room_type = input$room_type,host_listings_count = input$host_listings_count,guests_included = input$guests_included,
           extra_people = input$extra_people,accommodates = input$accommodates,bathrooms = input$bathrooms,bedrooms = input$bedrooms,
           beds = input$beds,bed_type = input$bed_type))
    })
    output$text3 <-renderText({
      "How many Airbnb are there in your district?"
    })
    output$count <-renderTable({
      listings %>% group_by(neighbourhood_cleansed) %>% filter(neighbourhood_cleansed == input$neighbourhood_cleansed) %>% summarise(Freq = n())
    })
    output$text4 <- renderText({
      "The property type in your district:"
    })
    output$type <- renderTable({
      a = listings %>% group_by(neighbourhood_cleansed,property_type) %>% filter(neighbourhood_cleansed == input$neighbourhood_cleansed) %>% summarise(Freq = n(),) %>% arrange(desc(Freq))
      a[1:10,2:3]
    })
     d <- reactive({
    listings[listings$neighbourhood_cleansed == input$neighbourhood_cleansed,]
  })
    output$map <- renderLeaflet({
    leaflet(d()) %>%
  addTiles() %>%
  addMarkers(~longitude, ~latitude, labelOptions = labelOptions(noHide = F), clusterOptions = markerClusterOptions(), popup = paste0("<b> Host ID: </b>", listings$host_id , "<br/><b> Rating: </b>", listings$review_scores_rating, "<br> <b> Price: </b>", listings$price, "<br/><b> Room Type: </b>", listings$room_type, "<br/><b> Property Type: </b>", listings$property_type)) %>% 
  setView(lat = 39.9, lng = 116.38, zoom = 10) %>%
  addProviderTiles("CartoDB.Positron")
  })
    output$heat <- renderMapdeck({
    set_token("pk.eyJ1IjoidG9tbXl4dTk3IiwiYSI6ImNrZjUzMnJjczAxMGYycW96ZGhhN3kybzgifQ.eZ1HnCaedPi6Yj-n-I3H7w")
mapdeck(style = mapdeck_style('satellite-streets'), pitch = 30, zoom = 10) %>%
  add_heatmap(
    data = d()
    , lat = "latitude"
    , lon = "longitude"
    , weight = "price"
    , layer_id = "heatmap_layer"
    , colour_range = colourvalues::colour_values(1:6, palette = colourvalues::get_palette("viridis")[70:256,])
  )
  })
})

shinyApp(ui, server)
?mapdeck_style
 
```
```{r}
#Customer Comment Word Cloud System

ui <- fluidPage(
  titlePanel("Customer Comment Word Cloud System"),
  sidebarLayout(
    sidebarPanel(
       textInput(inputId = "word",
                 label = "Find What Customers concern",
                 value = "comfortable"),
       sliderInput(inputId = "max_word",
                   label = "Maximum Number of Word",
                   value = "120",
                   min = 0,
                   max = 300)
    ),
    mainPanel(
      plotOutput("wordcloud")
  )
))
server <- function(input,output)({
  output$wordcloud <- renderPlot({
    p1 = word_vectors[input$word, ,drop = FALSE]
    cos_sim = sim2(x = word_vectors, y = p1, method = "cosine", norm = "l2")
    p1 = sort(cos_sim[,1], decreasing = TRUE)
    df = data.frame(item = as.character(names(p1)),freq = as.numeric(p1))
    df$item = gsub(",","",df$item)
    df = df[!duplicated(df$item), ]
    set.seed(1234)
    suppressWarnings(wordcloud(df$item, freq = df2$freq, scale = c(2,0.2),
                           max.words=input$max_word, random.order=FALSE, rot.per=0.2,
                           colors = c("#7db5b8", "#59c6f3", "#e06f69","#357b8a")))
  })
})

shinyApp(ui, server)



```
